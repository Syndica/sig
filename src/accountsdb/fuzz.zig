const std = @import("std");
const sig = @import("../sig.zig");
const zstd = @import("zstd");

const Account = sig.core.Account;
const ChannelPrintLogger = sig.trace.ChannelPrintLogger;
const Pubkey = sig.core.pubkey.Pubkey;
const Slot = sig.core.time.Slot;

const AccountDataHandle = sig.accounts_db.buffer_pool.AccountDataHandle;
const AccountsDB = sig.accounts_db.AccountsDB;
const FullSnapshotFileInfo = sig.accounts_db.snapshots.FullSnapshotFileInfo;
const IncrementalSnapshotFileInfo = sig.accounts_db.snapshots.IncrementalSnapshotFileInfo;

const N_RANDOM_THREADS = 8;

pub const TrackedAccount = struct {
    pubkey: Pubkey,
    slot: u64,
    data: [32]u8,

    pub fn initRandom(random: std.Random, slot: Slot) TrackedAccount {
        var data: [32]u8 = undefined;
        random.bytes(&data);
        return .{
            .pubkey = Pubkey.initRandom(random),
            .slot = slot,
            .data = data,
        };
    }

    pub fn toAccount(self: *const TrackedAccount, allocator: std.mem.Allocator) !Account {
        return .{
            .lamports = 19,
            .data = AccountDataHandle.initAllocatedOwned(try allocator.dupe(u8, &self.data)),
            .owner = Pubkey.ZEROES,
            .executable = false,
            .rent_epoch = 0,
        };
    }
};

pub fn run(seed: u64, args: *std.process.ArgIterator) !void {
    const maybe_max_actions_string = args.next();
    const maybe_max_actions = blk: {
        if (maybe_max_actions_string) |max_actions_str| {
            break :blk try std.fmt.parseInt(usize, max_actions_str, 10);
        } else {
            break :blk null;
        }
    };

    const N_ACCOUNTS_MAX: ?u64 = null;
    const N_ACCOUNTS_PER_SLOT = 10;

    var prng = std.Random.DefaultPrng.init(seed);
    const random = prng.random();

    var gpa_state: std.heap.DebugAllocator(.{
        .safety = true,
    }) = .init;
    defer _ = gpa_state.deinit();
    const allocator = gpa_state.allocator();

    var std_logger = try ChannelPrintLogger.init(.{
        .allocator = allocator,
        .max_level = .debug,
        .max_buffer = 1 << 20,
    }, null);
    defer std_logger.deinit();
    const logger = std_logger.logger();

    const use_disk = random.boolean();

    var fuzz_data_dir = try std.fs.cwd().makeOpenPath(sig.FUZZ_DATA_DIR, .{});
    defer fuzz_data_dir.close();

    const snapshot_dir_name = "accountsdb";
    var snapshot_dir = try fuzz_data_dir.makeOpenPath(snapshot_dir_name, .{});
    defer snapshot_dir.close();
    defer {
        // NOTE: sometimes this can take a long time so we print when we start and finish
        std.debug.print("deleting snapshot dir...\n", .{});
        fuzz_data_dir.deleteTreeMinStackSize(snapshot_dir_name) catch |err| {
            std.debug.print(
                "failed to delete snapshot dir ('{s}'): {}\n",
                .{ sig.utils.fmt.tryRealPath(snapshot_dir, "."), err },
            );
        };
        std.debug.print("deleted snapshot dir\n", .{});
    }
    std.debug.print("use disk: {}\n", .{use_disk});

    // CONTEXT: we need a separate directory to unpack the snapshot
    // generated by the accountsdb that's using the main directory,
    // since otherwise the alternate accountsdb may race with the
    // main one while reading/writing/deleting account files.
    var alternative_snapshot_dir = try snapshot_dir.makeOpenPath("alt", .{});
    defer alternative_snapshot_dir.close();

    var last_full_snapshot_validated_slot: Slot = 0;
    var last_inc_snapshot_validated_slot: Slot = 0;

    var accounts_db = try AccountsDB.init(.{
        .allocator = allocator,
        .logger = logger,
        .snapshot_dir = snapshot_dir,
        .geyser_writer = null,
        .gossip_view = null,
        .index_allocation = if (use_disk) .disk else .ram,
        .number_of_index_shards = sig.accounts_db.db.ACCOUNT_INDEX_SHARDS,
    });
    defer accounts_db.deinit();

    // prealloc some references to use throught the fuzz
    try accounts_db.account_index.expandRefCapacity(1_000_000);

    var manager_exit = std.atomic.Value(bool).init(false);
    const manager_handle = try std.Thread.spawn(.{}, sig.accounts_db.manager.runLoop, .{
        &accounts_db, sig.accounts_db.manager.ManagerLoopConfig{
            .exit = &manager_exit,
            .slots_per_full_snapshot = 50_000,
            .slots_per_incremental_snapshot = 5_000,
            .zstd_nb_workers = @intCast(std.Thread.getCpuCount() catch 0),
        },
    });
    defer {
        manager_exit.store(true, .release);
        manager_handle.join();
    }

    const Map = std.AutoArrayHashMap(Pubkey, TrackedAccount);
    var tracked_accounts_rw = sig.sync.RwMux(Map).init(blk: {
        var tracked_accounts = Map.init(allocator);
        try tracked_accounts.ensureTotalCapacity(10_000);
        break :blk tracked_accounts;
    });
    defer {
        const tracked_accounts, var tracked_accounts_lg = tracked_accounts_rw.writeWithLock();
        defer tracked_accounts_lg.unlock();
        tracked_accounts.deinit();
    }

    var threads: [N_RANDOM_THREADS]std.Thread = undefined;
    var reader_exit = std.atomic.Value(bool).init(true);
    var spawned_threads: u8 = 0;
    defer {
        reader_exit.store(true, .seq_cst);
        for (threads[0..spawned_threads]) |thread| thread.join();
    }

    // spawn the random reader threads
    for (&threads) |*thread| {
        // NOTE: these threads just access accounts and do not perform
        // any validation (in the .get block of the main fuzzer
        // loop, we perform validation)
        thread.* = try std.Thread.spawn(.{}, readRandomAccounts, .{
            &accounts_db,
            &tracked_accounts_rw,
            seed + spawned_threads,
            &reader_exit,
            spawned_threads,
        });
        spawned_threads += 1;

        std.debug.print("started readRandomAccounts thread: {}\n", .{spawned_threads});
    }
    std.debug.assert(spawned_threads == N_RANDOM_THREADS);

    const zstd_compressor = try zstd.Compressor.init(.{});
    defer zstd_compressor.deinit();

    var largest_rooted_slot: Slot = 0;
    var slot: Slot = 0;

    var pubkeys_this_slot = std.AutoHashMap(Pubkey, void).init(allocator);
    defer pubkeys_this_slot.deinit();

    // get/put a bunch of accounts
    while (true) {
        if (maybe_max_actions) |max_actions| {
            if (slot >= max_actions) {
                std.debug.print("reached max actions: {}\n", .{max_actions});
                break;
            }
        }
        defer slot += 1;

        const action = random.enumValue(enum { put, get });
        switch (action) {
            .put => {
                var update_all_existing = false;

                var accounts: [N_ACCOUNTS_PER_SLOT]Account = undefined;
                var pubkeys: [N_ACCOUNTS_PER_SLOT]Pubkey = undefined;

                {
                    const tracked_accounts, var tracked_accounts_lg =
                        tracked_accounts_rw.writeWithLock();
                    defer tracked_accounts_lg.unlock();

                    if (N_ACCOUNTS_MAX != null and tracked_accounts.count() > N_ACCOUNTS_MAX.?) {
                        // NOTE: we don't want to grow the db indefinitely -- so when we reach
                        // the max, we only update existing accounts
                        update_all_existing = true;
                    }

                    pubkeys_this_slot.clearRetainingCapacity();

                    for (&accounts, &pubkeys, 0..) |*account, *pubkey, i| {
                        errdefer for (accounts[0..i]) |prev_account| prev_account.deinit(allocator);

                        var tracked_account = TrackedAccount.initRandom(random, slot);

                        const existing_pubkey = random.boolean();
                        if ((existing_pubkey and tracked_accounts.count() > 0) or
                            update_all_existing)
                        {
                            const index = random.intRangeLessThan(
                                usize,
                                0,
                                tracked_accounts.count(),
                            );
                            const key = tracked_accounts.keys()[index];
                            // only if the pubkey is not already in this slot
                            if (!pubkeys_this_slot.contains(key)) {
                                tracked_account.pubkey = key;
                            }
                        }

                        account.* = try tracked_account.toAccount(allocator);
                        pubkey.* = tracked_account.pubkey;

                        // always overwrite the old slot
                        try tracked_accounts.put(tracked_account.pubkey, tracked_account);
                        try pubkeys_this_slot.put(pubkey.*, {});

                        // // NOTE: useful for debugging
                        // std.debug.print("put account @ slot {d}: {any}\n", .{ slot, tracked_account });
                    }
                }
                defer for (accounts) |account| account.deinit(allocator);

                // write to accounts_db
                try accounts_db.putAccountSlice(
                    &accounts,
                    &pubkeys,
                    slot,
                );
            },
            .get => {
                const key, const tracked_account = blk: {
                    const tracked_accounts, var tracked_accounts_lg =
                        tracked_accounts_rw.readWithLock();
                    defer tracked_accounts_lg.unlock();

                    const n_keys = tracked_accounts.count();
                    if (n_keys == 0) {
                        continue;
                    }
                    const index = random.intRangeAtMost(usize, 0, tracked_accounts.count() - 1);
                    const key = tracked_accounts.keys()[index];

                    break :blk .{ key, tracked_accounts.get(key).? };
                };
                const account, const ref =
                    try accounts_db.getAccountAndReference(&tracked_account.pubkey);
                defer account.deinit(allocator);

                if (!account.data.eqlSlice(&tracked_account.data)) {
                    std.debug.panic(
                        "found account {} with different data: " ++
                            "tracked: {any} vs found: {any} ({})\n",
                        .{ key, tracked_account.data, account.data, ref },
                    );
                }
            },
        }

        const create_new_root = random.boolean();
        if (create_new_root) {
            largest_rooted_slot = @min(slot, largest_rooted_slot + 2);
            accounts_db.largest_rooted_slot.store(largest_rooted_slot, .monotonic);
        }

        snapshot_validation: {
            // holding the lock here means that the snapshot archive(s) wont be deleted
            // since deletion requires a write lock
            const maybe_latest_snapshot_info, //
            var snapshot_info_lg //
            = accounts_db.latest_snapshot_gen_info.readWithLock();
            defer snapshot_info_lg.unlock();

            const snapshot_info = maybe_latest_snapshot_info.* orelse
                break :snapshot_validation; // no snapshot yet
            const full_snapshot_info = snapshot_info.full;

            // copy the archive to the alternative snapshot dir
            const full_snapshot_file_info: FullSnapshotFileInfo = full: {
                if (full_snapshot_info.slot <= last_full_snapshot_validated_slot) {
                    const inc_snapshot_info = snapshot_info.inc orelse break :snapshot_validation;
                    if (inc_snapshot_info.slot <= last_inc_snapshot_validated_slot) {
                        break :snapshot_validation;
                    }
                } else {
                    last_full_snapshot_validated_slot = full_snapshot_info.slot;
                }

                const full_snapshot_file_info: FullSnapshotFileInfo = .{
                    .slot = full_snapshot_info.slot,
                    .hash = full_snapshot_info.hash,
                };
                const full_archive_name_bounded = full_snapshot_file_info.snapshotArchiveName();
                const full_archive_name = full_archive_name_bounded.constSlice();

                const full_archive_file =
                    try snapshot_dir.openFile(full_archive_name, .{ .mode = .read_only });
                defer full_archive_file.close();

                try sig.accounts_db.snapshots.parallelUnpackZstdTarBall(
                    allocator,
                    .noop,
                    full_archive_file,
                    alternative_snapshot_dir,
                    5,
                    true,
                );
                logger.info().logf(
                    "fuzz[validate]: unpacked full snapshot '{s}'",
                    .{full_archive_name},
                );

                break :full full_snapshot_file_info;
            };

            // maybe copy the archive to the alternative snapshot dir
            const maybe_incremental_file_info: ?IncrementalSnapshotFileInfo = inc: {
                const inc_snapshot_info = snapshot_info.inc orelse break :inc null;

                // already validated
                if (inc_snapshot_info.slot <= last_inc_snapshot_validated_slot) break :inc null;
                last_inc_snapshot_validated_slot = inc_snapshot_info.slot;

                const inc_snapshot_file_info: IncrementalSnapshotFileInfo = .{
                    .base_slot = full_snapshot_info.slot,
                    .hash = inc_snapshot_info.hash,
                    .slot = inc_snapshot_info.slot,
                };
                const inc_archive_name_bounded = inc_snapshot_file_info.snapshotArchiveName();
                const inc_archive_name = inc_archive_name_bounded.constSlice();

                try snapshot_dir.copyFile(
                    inc_archive_name,
                    alternative_snapshot_dir,
                    inc_archive_name,
                    .{},
                );

                const inc_archive_file =
                    try alternative_snapshot_dir.openFile(inc_archive_name, .{});
                defer inc_archive_file.close();

                try sig.accounts_db.snapshots.parallelUnpackZstdTarBall(
                    allocator,
                    .noop,
                    inc_archive_file,
                    alternative_snapshot_dir,
                    5,
                    true,
                );
                logger.info().logf(
                    "fuzz[validate]: unpacked inc snapshot '{s}'",
                    .{inc_archive_name},
                );

                break :inc inc_snapshot_file_info;
            };

            const snapshot_files = sig.accounts_db.SnapshotFiles.fromFileInfos(
                full_snapshot_file_info,
                maybe_incremental_file_info,
            );

            const combined_manifest = try sig.accounts_db.FullAndIncrementalManifest.fromFiles(
                allocator,
                logger,
                alternative_snapshot_dir,
                snapshot_files,
            );
            defer combined_manifest.deinit(allocator);

            const index_type: AccountsDB.InitParams.Index =
                switch (accounts_db.account_index.reference_allocator) {
                    .disk => .disk,
                    .ram => .ram,
                    .parent => @panic("invalid argument"),
                };

            var alt_accounts_db = try AccountsDB.init(.{
                .allocator = allocator,
                .logger = .noop,
                .snapshot_dir = alternative_snapshot_dir,
                .geyser_writer = null,
                .gossip_view = null,
                .index_allocation = index_type,
                .number_of_index_shards = accounts_db.number_of_index_shards,
            });
            defer alt_accounts_db.deinit();

            (try alt_accounts_db.loadWithDefaults(
                allocator,
                combined_manifest,
                1,
                true,
                N_ACCOUNTS_PER_SLOT,
                false,
                false,
            )).deinit(allocator);

            const maybe_inc_slot = if (snapshot_info.inc) |inc| inc.slot else null;
            logger.info().logf(
                "loaded and validated snapshot at slot: {} (and inc snapshot @ slot {any})",
                .{ full_snapshot_info.slot, maybe_inc_slot },
            );
        }
    }

    std.debug.print("fuzzing complete\n", .{});
}

fn readRandomAccounts(
    db: *AccountsDB,
    tracked_accounts_rw: *sig.sync.RwMux(std.AutoArrayHashMap(Pubkey, TrackedAccount)),
    seed: u64,
    exit: *std.atomic.Value(bool),
    thread_id: usize,
) void {
    var prng = std.Random.DefaultPrng.init(seed);
    const random = prng.random();

    while (true) {
        if (exit.load(.seq_cst)) {
            std.debug.print("finishing readRandomAccounts thread: {}\n", .{thread_id});
            return;
        }

        var pubkeys: [50]Pubkey = undefined;
        {
            const tracked_accounts, var tracked_accounts_lg = tracked_accounts_rw.readWithLock();
            defer tracked_accounts_lg.unlock();

            const tracked_pubkeys = tracked_accounts.keys();
            if (tracked_pubkeys.len == 0) {
                // wait for some accounts to exist
                std.Thread.sleep(std.time.ns_per_s);
                continue;
            }

            for (&pubkeys) |*pubkey| pubkey.* = blk: {
                const index = random.intRangeLessThan(usize, 0, tracked_pubkeys.len);
                break :blk tracked_pubkeys[index];
            };
        }

        for (pubkeys) |pubkey| {
            const account = db.getAccount(&pubkey) catch continue;
            defer account.deinit(db.allocator);
        }
    }
}
